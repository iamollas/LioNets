{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LioNets Test Example\n",
    "Testing LioNets Architecture on SMS Spam Collection dataset and Hate Speech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.engine.saving import model_from_json\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, MaxPooling1D, Dropout, LSTM, RepeatVector, GlobalMaxPooling1D, \\\n",
    "    Concatenate, UpSampling2D, UpSampling1D, concatenate\n",
    "from keras.models import Model\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from load_dataset import Load_Dataset\n",
    "from IPython.display import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup process (predictor, encoder, decoder)\n",
    "We load the datasets using a python script to do preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X,y,class_names = Load_Dataset.load_hate_speech() #Uncomment to test LioNets on Hate Speech Dataset\n",
    "X,y,class_names = Load_Dataset.load_smsspam() #Uncomment to test LioNets on SMS Spam Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split in train and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test ,y_train ,y_test = train_test_split(X,y, random_state=70, stratify=y, test_size=0.33)\n",
    "X_train_copy = X_train.copy()\n",
    "X_test_copy = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As vectorizing technique we will use the TF-IDF vectorizer with maximum amount of words/features 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(max_features=4000)\n",
    "vec.fit(X_train_copy)\n",
    "X_train_copy = vec.transform(X_train_copy)\n",
    "X_test_copy = vec.transform(X_test_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will create and train the classifier (the predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vec.get_feature_names())\n",
    "encoder_input = Input(shape=(input_dim,))\n",
    "encoder_x = Dense(800, activation='tanh')(encoder_input)\n",
    "encoder_x = Dropout(0.2)(encoder_x)\n",
    "encoder_x = Dense(600, activation='tanh')(encoder_x)\n",
    "encoder_x = Dense(400, activation='tanh')(encoder_x)\n",
    "predictions = Dense(1, activation='sigmoid')(encoder_x)\n",
    "predictor = Model(encoder_input,predictions)\n",
    "predictor.compile(optimizer=\"adam\",loss=[\"binary_crossentropy\"],metrics=['accuracy'])\n",
    "print(predictor.summary())\n",
    "predictor.fit([X_train_copy], [y_train], validation_data=(X_test_copy,y_test), epochs=2, verbose=2)  # starts training\n",
    "y_preds = predictor.predict(X_test_copy)\n",
    "\n",
    "y_pred = [0 if a<0.5 else 1 for a in y_preds]\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our predictors architecture as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(predictor, 'Predictor.png',show_shapes=True)\n",
    "Image(retina=True, filename='Predictor.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract the encoder from the predictor. We will extract all the layers sequentially till the penultimate layer. Moreoverm we set the weights of the encoder untrainable in order to preserve the acquired knowledge, which the neural network inferred through its training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Sequential()\n",
    "for i in range(0,len(predictor.layers)-1):\n",
    "    print(predictor.layers[i])\n",
    "    encoder.add(predictor.layers[i])\n",
    "encoder.summary()\n",
    "encoder.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we form and we train the decoder model through an autoencoder. We will use the trained encoder as the first half of the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_dim = len(vec.get_feature_names())\n",
    "encoder_input = Input(shape=(input_dim,))\n",
    "encoded_x = encoder(encoder_input)\n",
    "decoder_x = Dense(600, activation='tanh')(encoded_x)\n",
    "decoder_x = Dense(800, activation='tanh')(decoder_x)\n",
    "decoder_x = Dropout(0.5)(decoder_x)\n",
    "decoded = Dense(input_dim, activation='softmax')(decoder_x)\n",
    "autoencoder = Model(encoder_input,decoded)\n",
    "autoencoder.compile(optimizer='adam',loss=[\"categorical_crossentropy\"],metrics=['accuracy'])#Did try MAE, MSY as well\n",
    "\n",
    "print(autoencoder.summary())\n",
    "autoencoder.fit([X_train_copy], [X_train_copy], validation_data=(X_test_copy,X_test_copy), epochs=150, verbose=2)  #Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_model(autoencoder, 'Autoencoder.png',show_shapes=True)\n",
    "Image(retina=True, filename='Autoencoder.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we extract the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder = Sequential()\n",
    "for i in range(2,len(autoencoder.layers)):\n",
    "    decoder.add(autoencoder.layers[i])\n",
    "decoder(optimizer='adam',loss=[\"categorical_crossentropy\"],metrics=['accuracy'])#Mae, mse try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(autoencoder, 'Decoder.png',show_shapes=True)\n",
    "Image(retina=True, filename='Decoder.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LioNets Experiments\n",
    "Having everything setted up, we are now ready to try our methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from LioNets import LioNet\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set an instance id to get predictions and produce an explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#idx = 5# For SMS Spam\n",
    "idx =10# For Hate Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize LioNets, giving the predictor, decoder, encoder, as well as the feature names, as arguments, we are ready to extract an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lionet = LioNet(model=predictor, autoencoder=None, decoder=decoder, encoder=encoder, feature_names=vec.get_feature_names())\n",
    "print(X_train[idx])\n",
    "lionet.explain_instance(X_train_copy[idx])\n",
    "lionet.print_neighbourhood_labels_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compare LioNets' explanation with LIME's explanation. So we set up LIME in order to produce One more explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def li_predict2(text):\n",
    "    texts = vec.transform(text)\n",
    "    a = predictor.predict(texts)\n",
    "    b = 1 - a \n",
    "    return np.column_stack((b,a))\n",
    "\n",
    "text = X_train[idx]\n",
    "split_expression = lambda s: re.split(r'\\W+', s)\n",
    "explainer = LimeTextExplainer(class_names=class_names, split_expression=split_expression)\n",
    "explanation = explainer.explain_instance(text_instance=text, classifier_fn=li_predict2)\n",
    "weights = OrderedDict(explanation.as_list())\n",
    "lime_w = pd.DataFrame({'Features': list(weights.keys()), \"Features' Weights\" : list(weights.values())})\n",
    "plt.figure(num=None, figsize=(6, 6), dpi=200, facecolor='w', edgecolor='k')\n",
    "lime_w = lime_w.sort_values(by=\"Features' Weights\", ascending=False)\n",
    "sns.barplot(x=\"Features' Weights\", y=\"Features\", data=lime_w)\n",
    "plt.xticks(rotation=90)\n",
    "print('Instance:',X_train[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vizualize the LIME's explanation with LIME's tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explanation.save_to_file('/tmp/oi.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbours Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to compute the distances between neighbours on the original space and in the reduced space. Firstly, we will apply this to an instance of train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we use the encoder to encode our data to the encoded space. Thus, we are reducing their dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_X_train = encoder.predict(X_train_copy)\n",
    "encoded_X_test = encoder.predict(X_test_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoded_X_train = decoder.predict(encoded_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set an instance id to get predictions and produce an explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ida = 5# For SMS Spam\n",
    "ida =10# For Hate Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the instance with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_instance = encoded_X_train[ida] #Also we take the encoded instance\n",
    "decoded_instance = decoder.predict(encoded_X_train)[ida] #And the decoded instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the decoded instance to see the performance of the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_instance_cleaned = []\n",
    "for i in decoded_instance:\n",
    "    if i < 0.01: decoded_instance_cleaned.append(0)\n",
    "    else: decoded_instance_cleaned.append(i)\n",
    "print(\" \".join(vec.inverse_transform([decoded_instance_cleaned])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the below metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the highest elemend and its index in the vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = list(X_train_copy[ida].copy().A[0]).index(X_train_copy[ida].copy().A[0].max())\n",
    "print(list(X_train_copy[ida].copy().A[0]).index(X_train_copy[ida].copy().A[0].max()))#Index of highest value\n",
    "initial_instance = X_train_copy[ida].copy().A[0]\n",
    "generated_instance = X_train_copy[ida].copy().A[0]\n",
    "print(\"Value on initial instance:\",generated_instance[max]) #Highest value\n",
    "generated_instance[max] = 0\n",
    "print(\"Value on generated instance:\",generated_instance[max]) #New value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the distances between the initial and the generated instance. They only differ in one feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Euclidean Distance:\",euclidean_distances([initial_instance],[generated_instance])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a neighbourhood and find the distances between them in the original space (That's what LIME does but only by putting one zero at a time). This will create only n neighbours (where n the number of words of the sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosSim = 0\n",
    "cosDis = 0\n",
    "eucDis = 0\n",
    "manDis = 0\n",
    "neighbours = []\n",
    "count = 0\n",
    "for i in range(0,len(initial_instance)):\n",
    "    if(initial_instance[i]!=0):\n",
    "        gen = initial_instance.copy()\n",
    "        gen[i]=0\n",
    "        eucDis = eucDis + euclidean_distances([initial_instance],[gen])[0][0]\n",
    "        count = count + 1\n",
    "        neighbours.append(gen)\n",
    "print(\"Euclidean Distance:\",eucDis/count)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode them and reduce their dimensions to 400! And Let's compute the new distances between the initial and the generated instance in the reduced space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_encoded_instance = encoded_X_train[ida]\n",
    "generated_encoded_instance = encoder.predict(np.array([generated_instance]))[0]\n",
    "print(\"Euclidean Distance:\",euclidean_distances([initial_encoded_instance],[generated_encoded_instance])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the distances in the reduced space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosSim = 0\n",
    "cosDis = 0\n",
    "eucDis = 0\n",
    "manDis = 0\n",
    "count = 0\n",
    "for i in neighbours:\n",
    "    generated_encoded_instance = encoder.predict(np.array([i]))[0]\n",
    "    eucDis = eucDis + euclidean_distances([initial_encoded_instance],[generated_encoded_instance])[0][0]\n",
    "    count = count + 1\n",
    "print(\"Euclidean Distance:\",eucDis/count)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the highest element and its index in the reduced vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = list(encoded_X_train[ida].copy()).index(encoded_X_train[ida].copy().max())\n",
    "print(list(encoded_X_train[ida].copy()).index(encoded_X_train[ida].copy().max()))\n",
    "initial_encoded_instance = encoded_X_train[ida].copy()\n",
    "generated_encoded_instance = encoded_X_train[ida].copy()\n",
    "print(generated_encoded_instance[max])\n",
    "generated_encoded_instance[max]=0\n",
    "print(generated_encoded_instance[max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Euclidean Distance:\",euclidean_distances([initial_encoded_instance],[generated_encoded_instance])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create neighbours (more than before in the reduced space) and we will compute their distances from the initial encoded instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosSim = 0\n",
    "cosDis = 0\n",
    "eucDis = 0\n",
    "manDis = 0\n",
    "neighbours = []\n",
    "count = 0\n",
    "for i in range(0,len(initial_encoded_instance)):\n",
    "    if(initial_encoded_instance[i]!=0):\n",
    "        gen = initial_encoded_instance.copy()\n",
    "        gen[i]=0\n",
    "        eucDis = eucDis + euclidean_distances([initial_encoded_instance],[gen])[0][0]\n",
    "        count = count + 1\n",
    "        neighbours.append(gen)\n",
    "print(\"Euclidean Distance:\",eucDis/count)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_instance = X_train_copy[ida].A[0]\n",
    "generated_instance = decoder.predict(np.array([generated_encoded_instance]))[0]\n",
    "print(\"Euclidean Distance:\",euclidean_distances([initial_instance],[generated_instance])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will find the differences on the original space after the transformation (through the decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosSim = 0\n",
    "cosDis = 0\n",
    "eucDis = 0\n",
    "manDis = 0\n",
    "count = 0\n",
    "for i in neighbours:\n",
    "    generated_decoded_instance = decoder.predict(np.array([i]))[0]\n",
    "    eucDis = eucDis + euclidean_distances([initial_instance],[generated_decoded_instance])[0][0]\n",
    "    count = count + 1\n",
    "print(\"Euclidean Distance:\",eucDis/count)\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
